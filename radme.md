# Predi√ß√£o de Pre√ßos de Casas - (Projeto Kaggle)

Este projeto faz parte do meu portf√≥lio de Ci√™ncia de Dados.  
Ele reproduz e documenta minha jornada de aprendizado ao participar do **Kaggle - House Prices: Advanced Regression Techniques**, um desafio cl√°ssico de regress√£o.

---

## üéØ Objetivo
Prever o pre√ßo final de venda de casas com base em dezenas de vari√°veis descritivas (num√©ricas e categ√≥ricas).  
O foco do projeto foi **experimentar, compreender e evoluir** modelos de machine learning supervisionados, comparando desempenho e interpretabilidade.

---

## üß≠ Etapas do Projeto

### 1. An√°lise Explorat√≥ria (EDA)
Explorei os dados buscando entender:
- Quais colunas t√™m maior impacto no pre√ßo (`OverallQual`, `GrLivArea`, `GarageCars`, etc.).
- A presen√ßa de outliers e valores ausentes.
- Rela√ß√µes entre vari√°veis num√©ricas (identificando multicolinearidade).

Criei gr√°ficos de distribui√ß√£o e correla√ß√£o para embasar as decis√µes de engenharia de features.

---

### 2. Engenharia de Features
Criei novas vari√°veis combinando informa√ß√µes relevantes, como:
- **`TotalSqFt`** = `GrLivArea` + `TotalBsmtSF`
- **`OverallQual_GarageCars`** = `OverallQual` √ó `GarageCars`

Essas features aumentaram a capacidade dos modelos simples de capturar rela√ß√µes complexas sem aumentar a dimensionalidade desnecessariamente.

---

### 3. Pr√©-processamento
Utilizei um **ColumnTransformer** para tratar os dados:
- **Num√©ricos:** imputa√ß√£o pela mediana e padroniza√ß√£o.
- **Categ√≥ricos:** imputa√ß√£o pelo valor mais frequente e OneHotEncoder com `handle_unknown='ignore'`.

Todo o processo foi encapsulado em pipelines, garantindo reprodutibilidade.

---

### 4. Modelagem
Testei e comparei tr√™s modelos principais:

| Modelo | Motivo da Escolha | Observa√ß√µes |
|--------|-------------------|--------------|
| **Decision Tree Regressor** | Primeiro baseline interpret√°vel | Forneceu insights r√°pidos sobre import√¢ncia de vari√°veis |
| **Random Forest Regressor** | Melhor estabilidade e redu√ß√£o de overfitting | Resultados mais consistentes ap√≥s otimiza√ß√£o de hiperpar√¢metros |
| **XGBoost Regressor** | Modelo final, captura rela√ß√µes n√£o-lineares com regulariza√ß√£o | Obteve melhor performance global |

---

### 5. Avalia√ß√£o
- M√©trica utilizada: **RMSLE** (Raiz do Erro Quadr√°tico M√©dio do Log).
- Avalia√ß√£o por **cross-validation (10 folds)** para estabilidade.
- Resultado de refer√™ncia: **RMSLE = 0.12926** no Kaggle.

---

### 6. Principais Aprendizados
- A import√¢ncia de iterar com modelos simples antes de partir para modelos complexos.
- O impacto real da engenharia de features bem pensada.
- A efici√™ncia de pipelines completas para garantir reprodutibilidade e clareza no c√≥digo.
- Como documentar um projeto t√©cnico de forma leg√≠vel para recrutadores e colegas de √°rea.

---

## üß© Tecnologias Utilizadas
- **Python 3.11**
- **Pandas**, **NumPy**, **Matplotlib**, **Seaborn**
- **Scikit-learn**
- **XGBoost**


